name: gpt2m_muon
method: grid
metric:
  name: val/loss
  goal: minimize

parameters:
  optim.learning_rate:
    values: [6e-4, 9e-4, 1.3e-3, 1.8e-3, 2.5e-3]

command:
  - ${env}
  - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
  - torchrun
  - --standalone
  - --nproc_per_node=4
  - train_gpt2.py
  - hydra.job.chdir=False
  - ${args_no_hyphens}

  # --- GPT-2 MEDIUM PRETRAINING (scratch) ---
  - init_from=scratch

  # GPT-2 medium architecture
  - model.n_layer=24
  - model.n_head=16
  - model.n_embd=1024

  # (optional but makes it explicit / GPT-2-like)
  - model.dropout=0.0

  # Sequence + batch settings (tune if needed)
  - block_size=1024
  - batch_size=40
  - gradient_accumulation_steps=12

  # optimizer
  - optim=muon
  - wandb_log=1
  - optim.decay=0

  # run length
  - max_iters=10000

  # output organization
  - exp=gpt2m_pretrain
  - "exp_name=gpt2m_muon_lr${optim.learning_rate}"
